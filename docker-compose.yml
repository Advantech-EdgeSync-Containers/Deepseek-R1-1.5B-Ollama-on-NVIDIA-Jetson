# ==========================================================================
# Jetson™ DeepSeek Ollama AI Development Docker Compose File
# ==========================================================================
# Version:      1.5.0
# Author:       Samir Singh <samir.singh@advantech.com> and Apoorv Saxena<apoorv.saxena@advantech.com>
# Last Updated: October 03, 2025
# 
# Description:
#   Multi-service configuration for DeepSeek R1 1.5B model deployment on
#   NVIDIA Jetson with Ollama inference engine and OpenWebUI frontend.
#
# Key Features:
#   • Ollama Inference Service:
#       – GPU-accelerated inference with native Ollama API
#       – Full NVIDIA hardware acceleration (CUDA, TensorRT, NVENC/NVDEC)
#       – Integrated WiseBench diagnostics and service startup scripts
#   
#   • OpenWebUI Frontend Service:
#       – Web-based chat interface with persistent storage
#       – Connects via Ollama OpenAI-compatible endpoint (/v1)
#       – Configurable port and generation features
#
# Services:
#   - deepseek-ollama-service: DeepSeek R1 1.5B with Ollama engine
#   - openweb-ui-service: Web interface for LLM interaction
#
# Volumes:
#   - ollama-models: Persistent Ollama model storage (/root/.ollama)
#   - open-webui: OpenWebUI data persistence
#
#
# Terms and Conditions:
#   1. Provided by Advantech Corporation "as is," without any express or
#      implied warranties of merchantability or fitness for a particular
#      purpose.
#   2. In no event shall Advantech Corporation be liable for any direct,
#      indirect, incidental, special, exemplary, or consequential damages
#      arising from the use of this software.
#   3. Redistribution and use in source and binary forms, with or without
#      modification, are permitted provided this notice appears in all
#      copies.
#
# Copyright (c) 2025 Advantech Corporation. All rights reserved.
# ==========================================================================
services:
  deepseek-ollama-service:
      image: edgesync.azurecr.io/advantech/deepseek-r-b-ollama-on-nvidia-jetson:1.6.0-Ubuntu22.04-ARM
      container_name: Deepseek-R1-1.5B-Ollama-on-NVIDIA-Jetson
      env_file: .env
      privileged: true
      network_mode: host
      runtime: nvidia
      tty: true
      stdin_open: true
      entrypoint: ["/bin/bash"]
      restart: always
      labels:
        maintainer: "Samir Singh <samir.singh@advantech.com>"
        vendor: "Advantech"
        version: "1.2"
        description: "Jetson™ Deepseek Ollama AI Development Container"
      environment:
        - NVIDIA_VISIBLE_DEVICES=all
        - NVIDIA_DRIVER_CAPABILITIES=all,compute,video,utility,graphics
      volumes:
        - ./start_services.sh:/workspace/start_services.sh
        - ./wise-bench.sh:/workspace/wise-bench.sh
        - ollama-models:/root/.ollama
        - /etc/nv_tegra_release:/etc/nv_tegra_release
        - /usr/lib/aarch64-linux-gnu/tegra:/usr/lib/aarch64-linux-gnu/tegra
        - /usr/src/jetson_multimedia_api:/usr/src/jetson_multimedia_api
        - /usr/lib/aarch64-linux-gnu/gstreamer-1.0:/usr/lib/aarch64-linux-gnu/gstreamer-1.0
        - /usr/local/cuda:/usr/local/cuda
      devices:
        - /dev/nvhost-ctrl
        - /dev/nvhost-ctrl-gpu
        - /dev/nvhost-prof-gpu
        - /dev/nvmap
        - /dev/nvhost-gpu
        - /dev/nvhost-as-gpu
        - /dev/nvhost-vic
        - /dev/nvhost-msenc
        - /dev/nvhost-nvdec
        - /dev/nvhost-nvjpg
        - /dev/nvgpu/igpu0
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                capabilities: [gpu, compute, utility, video, graphics]
                count: all

  openweb-ui-service:
    image: ghcr.io/open-webui/open-webui:0.6.5
    container_name: openweb-ui-service
    env_file: .env
    network_mode: host
    environment:
      OPENAI_API_BASE_URL: ${OPENAI_API_OLLAMA_BASE}/v1
      PORT: ${OPENWEBUI_PORT}
      ENABLE_TAGS_GENERATION: ${ENABLE_TAGS_GENERATION}
      ENABLE_TITLE_GENERATION: ${ENABLE_TITLE_GENERATION}
    volumes:
      - open-webui:/app/backend/data
    restart: always

volumes:
  open-webui:
    driver: local
  ollama-models:
    driver: local
